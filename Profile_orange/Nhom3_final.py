import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
import streamlit as st
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
import statsmodels.api as sm

# ƒê·ªçc d·ªØ li·ªáu
@st.cache_data
def load_data():
    df = pd.read_csv('Orange Quality Data.csv')
    # ƒê·ªïi t√™n c·ªôt sang ti·∫øng Vi·ªát
    df.columns = ['K√≠ch th∆∞·ªõc (cm)', 'Tr·ªçng l∆∞·ª£ng (g)', 'ƒê·ªô ng·ªçt (Brix)', 'ƒê·ªô chua (pH)', 
                  'ƒê·ªô m·ªÅm (1-5)', 'Th·ªùi gian thu ho·∫°ch (ng√†y)', 'ƒê·ªô ch√≠n (1-5)', 
                  'M√†u s·∫Øc', 'Gi·ªëng', 'Khuy·∫øt t·∫≠t (C/K)', 'Ch·∫•t l∆∞·ª£ng (1-5)']
    return df

# T·∫°o c√°c t√≠nh nƒÉng m·ªõi
def create_features(df):
    df['T·ªâ l·ªá ƒë∆∞·ªùng/axit'] = df['ƒê·ªô ng·ªçt (Brix)'] / df['ƒê·ªô chua (pH)']
    df['Ch·ªâ s·ªë tr∆∞·ªüng th√†nh'] = df['ƒê·ªô ch√≠n (1-5)'] * df['Th·ªùi gian thu ho·∫°ch (ng√†y)']
    df['M·∫≠t ƒë·ªô'] = df['Tr·ªçng l∆∞·ª£ng (g)'] / (df['K√≠ch th∆∞·ªõc (cm)'] ** 3)
    return df

# Chu·∫©n b·ªã d·ªØ li·ªáu
def prepare_data(df):
    df = create_features(df)
    numeric_df = df.select_dtypes(include=[np.number])
    X = numeric_df.drop('Ch·∫•t l∆∞·ª£ng (1-5)', axis=1)
    y = numeric_df['Ch·∫•t l∆∞·ª£ng (1-5)']
    return X, y

# H√†m ph√¢n lo·∫°i cam
def classify_orange(size, weight, brix, ph):
    if brix < 6 or ph > 4.4 or size < 6 or weight < 100:
        return "Cam h∆∞"
    elif size >= 8 and weight >= 200:
        if brix >= 11 and ph <= 3.5:
            return "Cam to ng·ªçt"
        else:
            return "Cam to chua"
    elif size < 8 or weight < 200:
        if brix >= 11 and ph <= 3.5:
            return "Cam nh·ªè ng·ªçt"
        else:
            return "Cam nh·ªè chua"
    else:
        return "Cam c√¢n b·∫±ng"

# Nh·∫≠p d·ªØ li·ªáu t·ª´ ng∆∞·ªùi d√πng
def user_input_features():
    size = st.sidebar.slider('K√≠ch th∆∞·ªõc (cm)', 6.0, 10.0, 8.0, key="size_slider")
    weight = st.sidebar.slider('Tr·ªçng l∆∞·ª£ng (g)', 100, 300, 200, key="weight_slider")
    brix = st.sidebar.slider('ƒê·ªô ng·ªçt (Brix)', 6.0, 16.0, 11.0, key="brix_slider")
    ph = st.sidebar.slider('ƒê·ªô chua (pH)', 2.8, 4.4, 3.5, key="ph_slider")
    softness = st.sidebar.slider('ƒê·ªô m·ªÅm (1-5)', 1, 5, 3, key="softness_slider")
    harvest_time = st.sidebar.slider('Th·ªùi gian thu ho·∫°ch (ng√†y)', 4, 25, 14, key="harvest_time_slider")
    ripeness = st.sidebar.slider('ƒê·ªô ch√≠n (1-5)', 1, 5, 3, key="ripeness_slider")
    
    data = {'K√≠ch th∆∞·ªõc (cm)': size,
            'Tr·ªçng l∆∞·ª£ng (g)': weight,
            'ƒê·ªô ng·ªçt (Brix)': brix,
            'ƒê·ªô chua (pH)': ph,
            'ƒê·ªô m·ªÅm (1-5)': softness,
            'Th·ªùi gian thu ho·∫°ch (ng√†y)': harvest_time,
            'ƒê·ªô ch√≠n (1-5)': ripeness}
    features = pd.DataFrame(data, index=[0])
    
    # T·∫°o c√°c t√≠nh nƒÉng m·ªõi
    features = create_features(features)
    
    return features

# H√†m t√¨m hyperparameters t·ªëi ∆∞u v√† hu·∫•n luy·ªán m√¥ h√¨nh
def optimize_model(model, param_grid, X_train, y_train):
    try:
        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        return grid_search.best_estimator_
    except Exception as e:
        st.warning(f"L·ªói khi t·ªëi ∆∞u h√≥a m√¥ h√¨nh: {str(e)}")
        return model

# H√†m ƒë√°nh gi√° m√¥ h√¨nh
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    cv_mse = -cv_scores.mean()
    return {'MSE': mse, 'MAE': mae, 'R2': r2, 'CV MSE': cv_mse}

# Hu·∫•n luy·ªán m√¥ h√¨nh
@st.cache_resource
def train_models(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    param_grids = {
        'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},
        'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},
        'SVR': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},
        'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},
        'Neural Network': {
            'hidden_layer_sizes': [(50,), (100,), (50, 50)],
            'alpha': [0.0001, 0.001, 0.01],
            'learning_rate_init': [0.001, 0.01, 0.1]
        }
    }
    
    models = {
        'Random Forest': RandomForestRegressor(random_state=42),
        'KNN': KNeighborsRegressor(),
        'Linear Regression': LinearRegression(),
        'SVR': SVR(),
        'Gradient Boosting': GradientBoostingRegressor(random_state=42),
        'Neural Network': MLPRegressor(max_iter=5000, random_state=42, early_stopping=True, validation_fraction=0.1)
    }
    
    results = {}
    for name, model in models.items():
        if name != 'Linear Regression':
            model = optimize_model(model, param_grids[name], X_train_scaled, y_train)
        results[name] = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)
        models[name] = model
    
    return scaler, models, results, X_train_scaled, X_test_scaled, y_train, y_test

# ƒê·ªãnh nghƒ©a c√°c emoji cho m·ªói m√¥ h√¨nh
model_icons = {
    'Random Forest': 'üå≥',
    'KNN': 'üîç',
    'Linear Regression': 'üìä',
    'SVR': 'üéØ',
    'Gradient Boosting': 'üöÄ',
    'Neural Network': 'üß†'
}

def tune_best_model(X_train, y_train, X_test, y_test, best_model_name, models):
    best_model = models[best_model_name]
    
    if best_model_name == 'Random Forest':
        param_distributions = {
            'n_estimators': randint(100, 500),
            'max_depth': [None] + list(randint(10, 100).rvs(10)),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10)
        }
    elif best_model_name == 'Gradient Boosting':
        param_distributions = {
            'n_estimators': randint(100, 500),
            'learning_rate': uniform(0.01, 0.3),
            'max_depth': randint(3, 10),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10)
        }
    elif best_model_name == 'Neural Network':
        param_distributions = {
            'hidden_layer_sizes': [(randint(50, 200).rvs(), randint(50, 200).rvs()) for _ in range(10)],
            'alpha': uniform(0.0001, 0.01),
            'learning_rate_init': uniform(0.001, 0.1)
        }
    else:
        st.warning(f"Kh√¥ng c√≥ c·∫•u h√¨nh tinh ch·ªânh cho m√¥ h√¨nh {best_model_name}")
        return best_model, None

    random_search = RandomizedSearchCV(best_model, param_distributions, n_iter=50, cv=5, 
                                       scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)
    random_search.fit(X_train, y_train)
    
    tuned_model = random_search.best_estimator_
    
    # Evaluate tuned model
    y_pred_tuned = tuned_model.predict(X_test)
    mse_tuned = mean_squared_error(y_test, y_pred_tuned)
    r2_tuned = r2_score(y_test, y_pred_tuned)
    
    return tuned_model, {'MSE': mse_tuned, 'R2': r2_tuned}

def display_tuning_results(best_model_name, original_results, tuned_results=None):
    # Th√™m d√≤ng ch·ªâ ra m√¥ h√¨nh t·ªët nh·∫•t
    st.write(f"**M√¥ h√¨nh t·ªët nh·∫•t l√†: {best_model_name}**")

    st.subheader(f'K·∫øt qu·∫£ cho m√¥ h√¨nh {best_model_name}')
    
    # T·∫°o DataFrame ƒë·ªÉ so s√°nh v·ªõi c√°c gi√° tr·ªã s·ªë
    comparison_df = pd.DataFrame({
        'M√¥ h√¨nh g·ªëc': [original_results['MSE']/25*100, original_results['R2']*100]
    }, index=['MSE (%)', 'R2 (%)'])
    
    if tuned_results:
        comparison_df['M√¥ h√¨nh tinh ch·ªânh'] = [tuned_results['MSE']/25*100, tuned_results['R2']*100]
        
        # T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm c·∫£i thi·ªán
        mse_improvement = (original_results['MSE'] - tuned_results['MSE']) / original_results['MSE'] * 100
        r2_improvement = (tuned_results['R2'] - original_results['R2']) / original_results['R2'] * 100
        
        st.write(f"C·∫£i thi·ªán MSE: {mse_improvement:.2f}%")
        st.write(f"C·∫£i thi·ªán R2: {r2_improvement:.2f}%")
    else:
        st.write("M√¥ h√¨nh n√†y kh√¥ng ƒë∆∞·ª£c tinh ch·ªânh.")
    
    # Hi·ªÉn th·ªã b·∫£ng so s√°nh v·ªõi t·ª∑ l·ªá ph·∫ßn trƒÉm ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng
    st.table(comparison_df.style.format("{:.2f}%"))
    
    # T·∫°o bi·ªÉu ƒë·ªì thanh ƒë·ªÉ so s√°nh
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    comparison_df.plot(kind='bar', ax=ax1, ylabel='MSE (%)')
    ax1.set_title('So s√°nh MSE')
    ax1.legend(title='')
    ax1.set_ylim(0, 100)  # ƒê·∫∑t gi·ªõi h·∫°n tr·ª•c y cho MSE
    
    comparison_df.plot(kind='bar', ax=ax2, ylabel='R2 (%)')
    ax2.set_title('So s√°nh R2')
    ax2.legend(title='')
    ax2.set_ylim(0, 100)  # ƒê·∫∑t gi·ªõi h·∫°n tr·ª•c y cho R2
        
# Th√™m nh√£n ph·∫ßn trƒÉm tr√™n ƒë·∫ßu m·ªói thanh
    for ax in [ax1, ax2]:
        for container in ax.containers:
            ax.bar_label(container, fmt='%.2f%%')
    
    plt.tight_layout()
    st.pyplot(fig)
    
#Ph√¢n t√≠ch chi ti·∫øt cho m√¥ h√¨nh 
def detailed_model_analysis(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    residuals = y_test - y_pred
    
    st.subheader(f'Ph√¢n t√≠ch chi ti·∫øt cho m√¥ h√¨nh {model_name}')
    
    # Metrics
    st.write(f"Mean Squared Error (MSE): {mse/25*100:.2f}%")
    st.write(f"Mean Absolute Error (MAE): {mae/5*100:.2f}%")
    st.write(f"R-squared (R2) Score: {r2*100:.2f}%")
    
    # Residual Plot
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(y_pred, residuals)
    ax.set_xlabel('Gi√° tr·ªã D·ª± ƒëo√°n')
    ax.set_ylabel('Residuals')
    ax.set_title('Bi·ªÉu ƒë·ªì Residual')
    ax.axhline(y=0, color='r', linestyle='--')
    st.pyplot(fig)
    
    # Q-Q Plot
    fig, ax = plt.subplots(figsize=(10, 6))
    sm.qqplot(residuals, ax=ax, line='45')
    ax.set_title('Q-Q Plot c·ªßa Residuals')
    st.pyplot(fig)
    
    # Histogram c·ªßa Residuals
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(residuals, bins=30)
    ax.set_xlabel('Residuals')
    ax.set_ylabel('T·∫ßn su·∫•t')
    ax.set_title('Histogram c·ªßa Residuals')
    st.pyplot(fig)
    
    # Feature Importance (n·∫øu c√≥)
    if hasattr(model, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': X_test.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x='importance', y='feature', data=feature_importance, ax=ax)
        ax.set_title('Feature Importance')
        st.pyplot(fig)
    
    # Ghi ch√∫ gi·∫£i th√≠ch
    st.info("""
    Gi·∫£i th√≠ch c√°c ch·ªâ s·ªë:
    
    1. Mean Squared Error (MSE): Th·ªÉ hi·ªán trung b√¨nh b√¨nh ph∆∞∆°ng c·ªßa sai s·ªë. Gi√° tr·ªã c√†ng th·∫•p c√†ng t·ªët. 
       MSE ƒë∆∞·ª£c t√≠nh l√† ph·∫ßn trƒÉm c·ªßa gi√° tr·ªã t·ªëi ƒëa c√≥ th·ªÉ (25 cho thang ƒëi·ªÉm 1-5).
    
    2. Mean Absolute Error (MAE): Th·ªÉ hi·ªán trung b√¨nh c·ªßa gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa sai s·ªë. Gi√° tr·ªã c√†ng th·∫•p c√†ng t·ªët.
       MAE ƒë∆∞·ª£c t√≠nh l√† ph·∫ßn trƒÉm c·ªßa thang ƒëi·ªÉm t·ªëi ƒëa (5).
    
    3. R-squared (R2) Score: Th·ªÉ hi·ªán ph·∫ßn trƒÉm bi·∫øn thi√™n c·ªßa bi·∫øn ph·ª• thu·ªôc ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi m√¥ h√¨nh. 
       Gi√° tr·ªã c√†ng g·∫ßn 100% c√†ng t·ªët.
    
    C√°c bi·ªÉu ƒë·ªì:
    - Bi·ªÉu ƒë·ªì Residual: ƒêi·ªÉm ph√¢n t√°n ƒë·ªÅu quanh ƒë∆∞·ªùng y=0 cho th·∫•y m√¥ h√¨nh c√≥ ƒë·ªô ch√≠nh x√°c t·ªët.
    - Q-Q Plot: N·∫øu c√°c ƒëi·ªÉm n·∫±m g·∫ßn ƒë∆∞·ªùng ch√©o, residuals c√≥ ph√¢n ph·ªëi g·∫ßn v·ªõi ph√¢n ph·ªëi chu·∫©n.
    - Histogram c·ªßa Residuals: H√¨nh d·∫°ng chu√¥ng ƒë·ªëi x·ª©ng cho th·∫•y residuals c√≥ ph√¢n ph·ªëi g·∫ßn v·ªõi ph√¢n ph·ªëi chu·∫©n.
    """)
def main():
    st.title('D·ª± ƒëo√°n v√† Ph√¢n lo·∫°i Ch·∫•t l∆∞·ª£ng Cam')

    # ƒê·ªçc v√† chu·∫©n b·ªã d·ªØ li·ªáu
    df = load_data()
    X, y = prepare_data(df)

    # Hu·∫•n luy·ªán m√¥ h√¨nh
    scaler, models, results, X_train_scaled, X_test_scaled, y_train, y_test = train_models(X, y)

    # Ch·ªçn m√¥ h√¨nh v·ªõi icon
    st.sidebar.header('Ch·ªçn M√¥ h√¨nh D·ª± ƒëo√°n')
    model_options = [f"{model_icons[model]} {model}" for model in models.keys()]
    selected_model_with_icon = st.sidebar.selectbox('Ch·ªçn m√¥ h√¨nh', model_options, label_visibility="collapsed")
    selected_model = selected_model_with_icon.split(' ', 1)[1]  # L·∫•y t√™n m√¥ h√¨nh t·ª´ chu·ªói ƒë√£ ch·ªçn

    # Giao di·ªán ng∆∞·ªùi d√πng ƒë·ªÉ nh·∫≠p th√¥ng s·ªë
    st.sidebar.header('Nh·∫≠p th√¥ng s·ªë Cam')
    input_df = user_input_features()

    st.subheader('Th√¥ng s·ªë ƒë·∫ßu v√†o')
    st.write(input_df)

    # K·∫øt qu·∫£ d·ª± ƒëo√°n v√† ph√¢n lo·∫°i
    input_scaled = scaler.transform(input_df)
    prediction = models[selected_model].predict(input_scaled)[0]
    
    size = input_df['K√≠ch th∆∞·ªõc (cm)'].values[0]
    weight = input_df['Tr·ªçng l∆∞·ª£ng (g)'].values[0]
    brix = input_df['ƒê·ªô ng·ªçt (Brix)'].values[0]
    ph = input_df['ƒê·ªô chua (pH)'].values[0]
    classification = classify_orange(size, weight, brix, ph)
    
    st.subheader('K·∫øt qu·∫£ d·ª± ƒëo√°n')
    st.write(f"M√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn: {model_icons[selected_model]} {selected_model}")
    st.write(f"Ch·∫•t l∆∞·ª£ng cam ƒë∆∞·ª£c d·ª± ƒëo√°n l√†: {prediction:.2f}/5.00 ({prediction/5.0*100:.2f}%)")
    st.write(f"D·ª±a tr√™n c√°c th√¥ng s·ªë, cam n√†y ƒë∆∞·ª£c ph√¢n lo·∫°i l√†: {classification}")

    # ƒê·∫øm s·ªë l∆∞·ª£ng cam trong m·ªói lo·∫°i
    df['Ph√¢n lo·∫°i'] = df.apply(lambda row: classify_orange(
        row['K√≠ch th∆∞·ªõc (cm)'], 
        row['Tr·ªçng l∆∞·ª£ng (g)'], 
        row['ƒê·ªô ng·ªçt (Brix)'], 
        row['ƒê·ªô chua (pH)']), axis=1)
    classification_counts = df['Ph√¢n lo·∫°i'].value_counts()
    
    st.subheader('S·ªë l∆∞·ª£ng Cam theo Ph√¢n lo·∫°i')
    st.write(classification_counts)

    # Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi ph√¢n lo·∫°i
    st.subheader('Bi·ªÉu ƒë·ªì Ph√¢n lo·∫°i')
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.barplot(x=classification_counts.index, y=classification_counts.values, ax=ax)
    plt.title('Bi·ªÉu ƒë·ªì Ph√¢n lo·∫°i Cam')
    plt.xlabel('Ph√¢n lo·∫°i')
    plt.ylabel('S·ªë l∆∞·ª£ng')
    plt.xticks(rotation=45)
    st.pyplot(fig)

    # K·∫øt qu·∫£ training (d·ª± ƒëo√°n chi ti·∫øt)
    st.subheader('K·∫øt qu·∫£ Training')

    # D·ª± ƒëo√°n chi ti·∫øt
    for name, model in models.items():
        prediction = model.predict(input_scaled)
        st.write(f'{name}: {prediction[0]:.2f}/5.0 ({prediction[0]/5.0*100:.2f}%)')

    # T·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng
    st.subheader('T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c t√≠nh')
    feature_importance = pd.DataFrame({
        'ƒê·∫∑c t√≠nh': X.columns,
        'Random Forest': models['Random Forest'].feature_importances_,
        'Gradient Boosting': models['Gradient Boosting'].feature_importances_
    })

    # Th√™m t·∫ßm quan tr·ªçng c·ªßa Linear Regression
    lr_importance = np.abs(models['Linear Regression'].coef_)
    feature_importance['Linear Regression'] = lr_importance / np.sum(lr_importance)

    feature_importance = feature_importance.sort_values('Random Forest', ascending=False)

    fig, ax = plt.subplots(figsize=(12, 8))
    feature_importance.plot(x='ƒê·∫∑c t√≠nh', y=['Random Forest', 'Gradient Boosting', 'Linear Regression'], kind='bar', ax=ax)
    plt.title('T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c t√≠nh')
    plt.xlabel('ƒê·∫∑c t√≠nh')
    plt.ylabel('T·∫ßm quan tr·ªçng')
    plt.legend(title='M√¥ h√¨nh')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    st.pyplot(fig)

    # Th√™m gi·∫£i th√≠ch v·ªÅ t·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c t√≠nh
    st.info("""
    Gi·∫£i th√≠ch v·ªÅ T·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c t√≠nh:

    1. √ù nghƒ©a: Bi·ªÉu ƒë·ªì n√†y th·ªÉ hi·ªán m·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng c·ªßa t·ª´ng ƒë·∫∑c t√≠nh ƒë·∫øn d·ª± ƒëo√°n ch·∫•t l∆∞·ª£ng cam trong c√°c m√¥ h√¨nh kh√°c nhau.

    2. C√°ch ƒë·ªçc bi·ªÉu ƒë·ªì:
    - Tr·ª•c x: Li·ªát k√™ c√°c ƒë·∫∑c t√≠nh c·ªßa cam.
    - Tr·ª•c y: Th·ªÉ hi·ªán m·ª©c ƒë·ªô quan tr·ªçng c·ªßa m·ªói ƒë·∫∑c t√≠nh (t·ª´ 0 ƒë·∫øn 1).
    - C√°c c·ªôt m√†u: M·ªói m√†u ƒë·∫°i di·ªán cho m·ªôt m√¥ h√¨nh kh√°c nhau.

    3. Di·ªÖn gi·∫£i:
    - ƒê·∫∑c t√≠nh c√≥ c·ªôt cao h∆°n c√≥ ·∫£nh h∆∞·ªüng l·ªõn h∆°n ƒë·∫øn d·ª± ƒëo√°n ch·∫•t l∆∞·ª£ng cam.
    - C√°c m√¥ h√¨nh kh√°c nhau c√≥ th·ªÉ ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c t√≠nh kh√°c nhau.
    
       ·ª®ng d·ª•ng:
    - Gi√∫p x√°c ƒë·ªãnh c√°c y·∫øu t·ªë quan tr·ªçng nh·∫•t ·∫£nh h∆∞·ªüng ƒë·∫øn ch·∫•t l∆∞·ª£ng cam.
    - C√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ t·ªëi ∆∞u h√≥a quy tr√¨nh tr·ªìng v√† chƒÉm s√≥c cam.
    - H·ªó tr·ª£ vi·ªác l·ª±a ch·ªçn ƒë·∫∑c t√≠nh khi thu th·∫≠p d·ªØ li·ªáu trong t∆∞∆°ng lai.

       L∆∞u √Ω: 
    - T·∫ßm quan tr·ªçng c√≥ th·ªÉ thay ƒë·ªïi gi·ªØa c√°c m√¥ h√¨nh.
    - N√™n xem x√©t k·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ nhi·ªÅu m√¥ h√¨nh ƒë·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán.
""")
     #Hi·ªÉn th·ªã top 3 ƒë·∫∑c t√≠nh quan tr·ªçng nh·∫•t
    top_features = feature_importance.nlargest(3, 'Random Forest')
    st.write("Top 3 ƒë·∫∑c t√≠nh quan tr·ªçng nh·∫•t (d·ª±a tr√™n Random Forest):")
    for index, row in top_features.iterrows():
        st.write(f"- {row['ƒê·∫∑c t√≠nh']}: {row['Random Forest']:.4f}")

    # Bi·ªÉu ƒë·ªì t∆∞∆°ng quan chi ti·∫øt
    st.subheader('Bi·ªÉu ƒë·ªì T∆∞∆°ng quan Chi ti·∫øt')
    corr = X.corr()
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(corr, 
                annot=True,
                fmt='.2f',
                cmap='RdBu_r',
                vmin=-1, vmax=1,
                square=True,
                linewidths=0.5,
                cbar=False)
    plt.title('Bi·ªÉu ƒë·ªì T∆∞∆°ng quan Chi ti·∫øt', fontsize=16)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    st.pyplot(fig)

    # Th√™m gi·∫£i th√≠ch v·ªÅ bi·ªÉu ƒë·ªì t∆∞∆°ng quan
    st.info("""
    Gi·∫£i th√≠ch v·ªÅ Bi·ªÉu ƒë·ªì T∆∞∆°ng quan Chi ti·∫øt:

    1. √ù nghƒ©a:
    - Bi·ªÉu ƒë·ªì n√†y th·ªÉ hi·ªán m·ª©c ƒë·ªô t∆∞∆°ng quan gi·ªØa c√°c ƒë·∫∑c t√≠nh c·ªßa cam.
    - T∆∞∆°ng quan dao ƒë·ªông t·ª´ -1 ƒë·∫øn 1, trong ƒë√≥:
        * 1 l√† t∆∞∆°ng quan d∆∞∆°ng ho√†n h·∫£o
        * -1 l√† t∆∞∆°ng quan √¢m ho√†n h·∫£o
        * 0 l√† kh√¥ng c√≥ t∆∞∆°ng quan

    2. C√°ch ƒë·ªçc bi·ªÉu ƒë·ªì:
    - M·ªói √¥ trong bi·ªÉu ƒë·ªì th·ªÉ hi·ªán t∆∞∆°ng quan gi·ªØa hai ƒë·∫∑c t√≠nh.
    - M√†u s·∫Øc: 
        * M√†u ƒë·ªè th·ªÉ hi·ªán t∆∞∆°ng quan d∆∞∆°ng
        * M√†u xanh th·ªÉ hi·ªán t∆∞∆°ng quan √¢m
        * M√†u c√†ng ƒë·∫≠m, t∆∞∆°ng quan c√†ng m·∫°nh
    - S·ªë trong m·ªói √¥ l√† gi√° tr·ªã c·ª• th·ªÉ c·ªßa h·ªá s·ªë t∆∞∆°ng quan.

    3. Di·ªÖn gi·∫£i:
    - T∆∞∆°ng quan g·∫ßn 1 ho·∫∑c -1: Hai ƒë·∫∑c t√≠nh c√≥ m·ªëi li√™n h·ªá m·∫°nh.
    - T∆∞∆°ng quan g·∫ßn 0: Hai ƒë·∫∑c t√≠nh √≠t ho·∫∑c kh√¥ng c√≥ li√™n h·ªá.
    - D·∫•u (+/-) ch·ªâ ra chi·ªÅu c·ªßa m·ªëi quan h·ªá (c√πng chi·ªÅu ho·∫∑c ng∆∞·ª£c chi·ªÅu).
            
    4. ·ª®ng d·ª•ng:
    - Hi·ªÉu r√µ m·ªëi quan h·ªá gi·ªØa c√°c ƒë·∫∑c t√≠nh c·ªßa cam.
    - Ph√°t hi·ªán c√°c ƒë·∫∑c t√≠nh c√≥ th·ªÉ d·ª± ƒëo√°n l·∫´n nhau.
    - H·ªó tr·ª£ trong vi·ªác l·ª±a ch·ªçn ƒë·∫∑c t√≠nh cho m√¥ h√¨nh d·ª± ƒëo√°n.

    5. L∆∞u √Ω: 
    - T∆∞∆°ng quan kh√¥ng ƒë·ªìng nghƒ©a v·ªõi quan h·ªá nh√¢n qu·∫£.
    - C·∫ßn xem x√©t √Ω nghƒ©a th·ª±c t·∫ø c·ªßa m·ªëi t∆∞∆°ng quan, kh√¥ng ch·ªâ d·ª±a v√†o s·ªë li·ªáu.
            """)
    # Hi·ªÉn th·ªã c√°c c·∫∑p ƒë·∫∑c t√≠nh c√≥ t∆∞∆°ng quan m·∫°nh nh·∫•t
    strong_correlations = corr.unstack().sort_values(key=abs, ascending=False)
    strong_correlations = strong_correlations[(strong_correlations != 1.0) & (abs(strong_correlations) > 0.5)]
    st.write("C√°c c·∫∑p ƒë·∫∑c t√≠nh c√≥ t∆∞∆°ng quan m·∫°nh (|r| > 0.5):")
    for (feature1, feature2), correlation in strong_correlations.items():
        if feature1 != feature2:
            st.write(f"- {feature1} v√† {feature2}: {correlation:.2f}")

    # B·∫£ng so s√°nh c√°c m√¥ h√¨nh
    st.subheader('So s√°nh c√°c M√¥ h√¨nh')
    comparison_df = pd.DataFrame(results).T

    # Chuy·ªÉn ƒë·ªïi MSE v√† MAE th√†nh ph·∫ßn trƒÉm so v·ªõi gi√° tr·ªã trung b√¨nh c·ªßa y
    y_mean = y.mean()
    comparison_df['MSE (%)'] = (comparison_df['MSE'] / (y_mean ** 2)) * 100
    comparison_df['MAE (%)'] = (comparison_df['MAE'] / y_mean) * 100

    # Chuy·ªÉn ƒë·ªïi R2 th√†nh ph·∫ßn trƒÉm
    comparison_df['R2 (%)'] = comparison_df['R2'] * 100

    # Chuy·ªÉn ƒë·ªïi CV MSE th√†nh ph·∫ßn trƒÉm
    comparison_df['CV MSE (%)'] = (comparison_df['CV MSE'] / (y_mean ** 2)) * 100

    # S·∫Øp x·∫øp theo MSE (%) tƒÉng d·∫ßn
    comparison_df = comparison_df.sort_values('MSE (%)')

    # Ch·ªçn c√°c c·ªôt c·∫ßn hi·ªÉn th·ªã
    display_columns = ['MSE (%)', 'MAE (%)', 'R2 (%)', 'CV MSE (%)']

    # T·∫°o b·∫£ng so s√°nh v·ªõi m√†u s·∫Øc
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.heatmap(comparison_df[display_columns], annot=True, cmap='YlGnBu', fmt='.2f', ax=ax)
    plt.title('So s√°nh c√°c M√¥ h√¨nh (%)', fontsize=16)
    plt.ylabel('M√¥ h√¨nh')
    plt.tight_layout()
    st.pyplot(fig)

    # Th√™m ghi ch√∫
    st.info("Ghi ch√∫: MSE v√† MAE th·∫•p h∆°n l√† t·ªët h∆°n, trong khi R2 cao h∆°n l√† t·ªët h∆°n.")

    # Hi·ªÉn th·ªã m√¥ h√¨nh t·ªët nh·∫•t
    best_model = comparison_df.index[0]
    st.write(f"M√¥ h√¨nh t·ªët nh·∫•t: {best_model}")
    st.write(f"MSE: {comparison_df.loc[best_model, 'MSE (%)']: .2f}%")
    st.write(f"R2: {comparison_df.loc[best_model, 'R2 (%)']: .2f}%")

    # Bi·ªÉu ƒë·ªì so s√°nh MSE v√† R2 c·ªßa c√°c m√¥ h√¨nh
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

    comparison_df['MSE (%)'].plot(kind='bar', ax=ax1)
    ax1.set_title('MSE c·ªßa c√°c m√¥ h√¨nh (%)')
    ax1.set_ylabel('MSE (%)')
    ax1.set_xlabel('M√¥ h√¨nh')

    comparison_df['R2 (%)'].plot(kind='bar', ax=ax2)
    ax2.set_title('R2 c·ªßa c√°c m√¥ h√¨nh (%)')
    ax2.set_ylabel('R2 (%)')
    ax2.set_xlabel('M√¥ h√¨nh')

    plt.tight_layout()
    st.pyplot(fig)

    # Th√™m ghi ch√∫ cho bi·ªÉu ƒë·ªì
    st.info("Ghi ch√∫: ƒê·ªëi v·ªõi bi·ªÉu ƒë·ªì MSE, c·ªôt th·∫•p h∆°n l√† t·ªët h∆°n. ƒê·ªëi v·ªõi bi·ªÉu ƒë·ªì R2, c·ªôt cao h∆°n l√† t·ªët h∆°n.")

    # Bi·ªÉu ƒë·ªì D·ª± ƒëo√°n vs Th·ª±c t·∫ø cho m·ªói m√¥ h√¨nh
    st.subheader('Bi·ªÉu ƒë·ªì D·ª± ƒëo√°n vs Th·ª±c t·∫ø')
    fig, axs = plt.subplots(3, 2, figsize=(20, 24))  # TƒÉng k√≠ch th∆∞·ªõc ƒë·ªÉ ch·ª©a 6 m√¥ h√¨nh
    for (name, model), ax in zip(models.items(), axs.ravel()):
        y_pred = model.predict(X_test_scaled)
        ax.scatter(y_test, y_pred, alpha=0.5)
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        ax.set_xlabel('Gi√° tr·ªã Th·ª±c t·∫ø')
        ax.set_ylabel('Gi√° tr·ªã D·ª± ƒëo√°n')
        ax.set_title(f'Bi·ªÉu ƒë·ªì D·ª± ƒëo√°n vs Th·ª±c t·∫ø - {name}')
        ax.text(0.05, 0.95, f'R2: {r2_score(y_test, y_pred):.2f}', transform=ax.transAxes, fontsize=12,
                verticalalignment='top')
    plt.tight_layout()
    st.pyplot(fig)

    # Th√™m ghi ch√∫ gi·∫£i th√≠ch
    st.info("""
    Ghi ch√∫ v·ªÅ Bi·ªÉu ƒë·ªì D·ª± ƒëo√°n vs Th·ª±c t·∫ø:
    - M·ªói ƒëi·ªÉm tr√™n bi·ªÉu ƒë·ªì ƒë·∫°i di·ªán cho m·ªôt m·∫´u cam trong t·∫≠p ki·ªÉm tra.
    - Tr·ª•c x th·ªÉ hi·ªán gi√° tr·ªã ch·∫•t l∆∞·ª£ng th·ª±c t·∫ø c·ªßa cam.
    - Tr·ª•c y th·ªÉ hi·ªán gi√° tr·ªã ch·∫•t l∆∞·ª£ng d·ª± ƒëo√°n b·ªüi m√¥ h√¨nh.
    - ƒê∆∞·ªùng ch√©o m√†u ƒë·ªè ƒë·ª©t n√©t th·ªÉ hi·ªán d·ª± ƒëo√°n ho√†n h·∫£o (gi√° tr·ªã d·ª± ƒëo√°n = gi√° tr·ªã th·ª±c t·∫ø).
    - C√°c ƒëi·ªÉm c√†ng g·∫ßn ƒë∆∞·ªùng ch√©o ƒë·ªè, d·ª± ƒëo√°n c√†ng ch√≠nh x√°c.
    - R2 (h·ªá s·ªë x√°c ƒë·ªãnh) c√†ng g·∫ßn 1, m√¥ h√¨nh c√†ng t·ªët.
    - M√¥ h√¨nh l√Ω t∆∞·ªüng s·∫Ω c√≥ c√°c ƒëi·ªÉm t·∫≠p trung s√°t ƒë∆∞·ªùng ch√©o ƒë·ªè v√† R2 g·∫ßn 1.
    """)
    # Ph·∫ßn tinh ch·ªânh m√¥ h√¨nh
    st.subheader('Tinh ch·ªânh M√¥ h√¨nh')

    if st.button('Hi·ªÉn th·ªã k·∫øt qu·∫£ m√¥ h√¨nh t·ªët nh·∫•t'):
        with st.spinner('ƒêang x·ª≠ l√Ω...'):
            best_model_name = comparison_df.index[0]
            original_results = {
                'MSE': comparison_df.loc[best_model_name, 'MSE'],
                'R2': comparison_df.loc[best_model_name, 'R2']
            }
            
            if best_model_name != 'Linear Regression':
                tuned_model, tuned_results = tune_best_model(X_train_scaled, y_train, X_test_scaled, y_test, best_model_name, models)
                
                if tuned_results:
                    display_tuning_results(best_model_name, original_results, tuned_results)
                    # S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ tinh ch·ªânh cho ph√¢n t√≠ch chi ti·∫øt
                    detailed_model_analysis(tuned_model, X_test_scaled, y_test, f"{best_model_name} (Tinh ch·ªânh)")
                else:
                    st.warning(f"Kh√¥ng th·ªÉ tinh ch·ªânh m√¥ h√¨nh {best_model_name}")
                    display_tuning_results(best_model_name, original_results)
                    # S·ª≠ d·ª•ng m√¥ h√¨nh g·ªëc cho ph√¢n t√≠ch chi ti·∫øt
                    detailed_model_analysis(models[best_model_name], X_test_scaled, y_test, best_model_name)
            else:
                st.info("M√¥ h√¨nh Linear Regression kh√¥ng c·∫ßn tinh ch·ªânh.")
                display_tuning_results(best_model_name, original_results)
                # Ph√¢n t√≠ch chi ti·∫øt cho Linear Regression
                detailed_model_analysis(models[best_model_name], X_test_scaled, y_test, best_model_name)
    

def display_designer_info():
    st.markdown("---")
    st.markdown("### Ng∆∞·ªùi thi·∫øt k·∫ø: Nh√≥m 3")
    st.markdown("Ph·∫°m Ph√∫c Khang-2275201140014")
    st.markdown("Ho√†ng Q√∫y C∆∞·ªùng-2275201140007")
    st.markdown("Ho√†ng Gia Anh-2275201140001")

if __name__ == '__main__':
    main()
    display_designer_info()